# --- MADDPG specific parameters ---
# --- pymarl options ---
name: "maddpg_pp"

runner: "episode" #Runs 1 env for an episode [episode | parallel]
learner: "maddpg_learner"
mac: cqmix_mac

env_args:
  state_last_action: False # critic adds last action internally
batch_size_run: 1 # Number of environments to run in parallel
test_nepisode: 10 # Number of episodes to test for
test_interval: 500000 # Test after {} time steps have passed
test_greedy: ~ # Use greedy evaluation (if False, will set epsilon floor to 0
log_interval: 10000 # Log summary of stats after every {} time steps
runner_log_interval: 10000 # Log runner stats (not test stats) every {} time steps
learner_log_interval: 10000 # Log training stats every {} time steps
t_max: 1200000 # totally number of time steps regardless episodes
use_cuda: True # Use gpu by default unless it isn't available


# --- Logging options ---
use_tensorboard: True # Log results to tensorboard
save_model: True # Save the models to disk
save_model_interval: 500000 # Save models after this many time steps
checkpoint_path: "" # Load a checkpoint from this path
evaluate: False # Evaluate model for test_n episode episodes and quit (no training)
load_step: 0 # Load model trained on this many timesteps (0 if choose max possible)
save_replay: False # Saving the replay of the model loaded from checkpoint_path
local_results_path: "results" # Path for local results

# --- RL hyper parameters ---
gamma: 0.98
batch_size: 128 # batch of episodes that sample for training
lr: 0.0001 # Learning rate for agents
critic_lr: 0.001 # Learning rate for critics
recurrent_critic: False

# --- Agent parameters ---

obs_agent_id: False # Include the agent's one_hot id in the observation
obs_last_action: False # Include the agent's last action (one_hot) in the observation and feed in Actor

# --- Experiment running params ---
repeat_id: 1
label: "default_label"
run_mode: ~ # "sequential"
n_runners: ~ #1
continuous_actions: True
discretize_actions: False
debug: False
n_train: 1
learn_interval: 1 # 100 #TODO: ?
testing_on: True
runner_scope: 'episodic' # episodic | transition
plot_loss_network: False
verbose: False

# ---Buffer
#buffer_mode: "transitions"  #TODO
buffer_cpu_only: True
buffer_size: 1000 # totally number of episodes the buffer can contain TODO：5000变成2000性能有所提升?
use_buffer_compression: False
buffer_device: ~
buffer_algo: ~
buffer_chunk_size: ~
buffer_warmup: 512 # wait for there are 100 episodes in buffer and start training

# --- Exploration
exploration_mode: "original" # original | gaussian | ornstein_uhlenbeck
act_noise: 0.3 # Stddev for Gaussian exploration noise added to policy at training time. TODO：为0.1时学不出来，为0.3时可以学出来
ou_theta: 0.15 # D
ou_sigma: 0.2 # D
ou_noise_scale: 0.3
final_ou_noise_scale: 0.
ou_stop_episode: 100 # training noise goes to zero after this episode TODO
start_steps: 0 # Number of steps for uniform-random action selection, before running real policy. Helps exploration. #TODO: 目前设为10000


# Actor
agent: mlp
rnn_hidden_dim: 64 # TODO： =256 使性能变好？
action_selector: ~
agent_output_type: ~ # pi_logits | ~

# Critic Update
td_lambda: 0.8
critic_train_reps: 1
q_nstep: 0  # 0 corresponds to default Q, 1 is r + gamma*Q, etc
double_q: False


# A-C update
optimizer: adam # rmsprop | adam
optimizer_epsilon: 0.00000001 # TODO 1E-8
target_update_mode: 'soft' # 'soft' | 'hard'
target_update_tau: 0.003 # TODO： 从0.005改成0.001曲线变得更稳定？？？
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp epsilon
grad_norm_clip: 10 # Reduce magnitude of gradients above this L2 norm



action_range: ~

epsilon_decay_mode: ~
epsilon_start: ~
epsilon_finish: ~
epsilon_anneal_time: ~
target_update_interval: ~

weight_decay: True
weight_decay_factor: 0.0001

agent_return_logits: False
q_embed_dim: 1